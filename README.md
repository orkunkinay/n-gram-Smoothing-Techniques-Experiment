# n-gram-Smoothing-Techniques-Experiment

## Paper Abstract:
Smoothing techniques are critical in Natural Language Processing (NLP) for addressing the issue of zero probabilities in language models. This research paper presents a comparative study of various smoothing techniques applied to n-gram models of different orders. The evaluation metric used is perplexity, which measures the effectiveness of these techniques. The study investigates the performance of Additive (Laplace) Smoothing, Good-Turing Smoothing, Jelinek-Mercer Smoothing, Kneser-Ney Smoothing, and their respective improvements across unigram, bigram, and trigram models.
